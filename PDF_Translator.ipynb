{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvjs-3oBNHWu",
        "outputId": "f05efef5-b539-4af2-a058-e0030c088e85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 transformers langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KdzhOK0Nu50"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langdetect import detect, DetectorFactory, lang_detect_exception\n",
        "from transformers import AutoTokenizer , AutoModelForSeq2SeqLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nIBW6qtOJb0"
      },
      "outputs": [],
      "source": [
        "#Setting up the key\n",
        "HUGGINGFACE_API_KEY = \"your key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evs3k2dROdMN"
      },
      "outputs": [],
      "source": [
        "#function to extract the text from the pdf\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A09HigvXOkd_"
      },
      "outputs": [],
      "source": [
        "#function to translate text using NLLB mdoel\n",
        "from transformers import AutoTokenizer, M2M100ForConditionalGeneration\n",
        "def translate_text_nllb(text, target_language):\n",
        "  '''tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-3.3B\", use_auth_token=HUGGINGFACE_API_KEY)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-3.3B\", use_auth_token=HUGGINGFACE_API_KEY)\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", padding = True)\n",
        "  translated_tokens = model.generate(\n",
        "      inputs[\"input_ids\"],\n",
        "      forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]\n",
        "  )\n",
        "  translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]'''\n",
        "\n",
        "  model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
        "\n",
        "  text_to_translate = text\n",
        "  model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n",
        "\n",
        "  # translate to French\n",
        "  gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(target_language ))\n",
        "  translated_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
        "  return translated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To9nONXSPezF"
      },
      "outputs": [],
      "source": [
        "#funtion to detect language\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        DetectorFactory.seed = 0\n",
        "        language = detect(text)\n",
        "        return language\n",
        "    except lang_detect_exception.LangDetectException:\n",
        "        return None\n",
        "# Map detected languages to NLLB-200 language codes\n",
        "\n",
        "lang_code_map = {\n",
        "    \"af\": \"afr_Latn\",\n",
        "    \"am\": \"amh_Ethi\",\n",
        "    \"ar\": \"arb_Arab\",\n",
        "    \"as\": \"asm_Beng\",\n",
        "    \"az\": \"aze_Latn\",\n",
        "    \"be\": \"bel_Cyrl\",\n",
        "    \"bg\": \"bul_Cyrl\",\n",
        "    \"bn\": \"ben_Beng\",\n",
        "    \"bs\": \"bos_Latn\",\n",
        "    \"ca\": \"cat_Latn\",\n",
        "    \"ceb\": \"ceb_Latn\",\n",
        "    \"cs\": \"ces_Latn\",\n",
        "    \"cy\": \"cym_Latn\",\n",
        "    \"da\": \"dan_Latn\",\n",
        "    \"de\": \"deu_Latn\",\n",
        "    \"el\": \"ell_Grek\",\n",
        "    \"en\": \"eng_Latn\",\n",
        "    \"es\": \"spa_Latn\",\n",
        "    \"et\": \"est_Latn\",\n",
        "    \"fa\": \"pes_Arab\",\n",
        "    \"fi\": \"fin_Latn\",\n",
        "    \"fr\": \"fra_Latn\",\n",
        "    \"gu\": \"guj_Gujr\",\n",
        "    \"ha\": \"hau_Latn\",\n",
        "    \"he\": \"heb_Hebr\",\n",
        "    \"hi\": \"hin_Deva\",\n",
        "    \"hr\": \"hrv_Latn\",\n",
        "    \"hu\": \"hun_Latn\",\n",
        "    \"hy\": \"hye_Armn\",\n",
        "    \"id\": \"ind_Latn\",\n",
        "    \"is\": \"isl_Latn\",\n",
        "    \"it\": \"ita_Latn\",\n",
        "    \"ja\": \"jpn_Jpan\",\n",
        "    \"jv\": \"jav_Latn\",\n",
        "    \"ka\": \"kat_Geor\",\n",
        "    \"kk\": \"kaz_Cyrl\",\n",
        "    \"km\": \"khm_Khmr\",\n",
        "    \"kn\": \"kan_Knda\",\n",
        "    \"ko\": \"kor_Hang\",\n",
        "    \"lo\": \"lao_Laoo\",\n",
        "    \"lt\": \"lit_Latn\",\n",
        "    \"lv\": \"lav_Latn\",\n",
        "    \"mg\": \"plt_Latn\",\n",
        "    \"mk\": \"mkd_Cyrl\",\n",
        "    \"ml\": \"mal_Mlym\",\n",
        "    \"mn\": \"khk_Cyrl\",\n",
        "    \"mr\": \"mar_Deva\",\n",
        "    \"ms\": \"zsm_Latn\",\n",
        "    \"my\": \"mya_Mymr\",\n",
        "    \"ne\": \"npi_Deva\",\n",
        "    \"nl\": \"nld_Latn\",\n",
        "    \"no\": \"nob_Latn\",\n",
        "    \"or\": \"ori_Orya\",\n",
        "    \"pa\": \"pan_Guru\",\n",
        "    \"pl\": \"pol_Latn\",\n",
        "    \"ps\": \"pus_Arab\",\n",
        "    \"pt\": \"por_Latn\",\n",
        "    \"ro\": \"ron_Latn\",\n",
        "    \"ru\": \"rus_Cyrl\",\n",
        "    \"si\": \"sin_Sinh\",\n",
        "    \"sk\": \"slk_Latn\",\n",
        "    \"sl\": \"slv_Latn\",\n",
        "    \"so\": \"som_Latn\",\n",
        "    \"sq\": \"sqi_Latn\",\n",
        "    \"sr\": \"srp_Cyrl\",\n",
        "    \"su\": \"sun_Latn\",\n",
        "    \"sv\": \"swe_Latn\",\n",
        "    \"ta\": \"tam_Taml\",\n",
        "    \"te\": \"tel_Telu\",\n",
        "    \"tg\": \"tgk_Cyrl\",\n",
        "    \"th\": \"tha_Thai\",\n",
        "    \"ti\": \"tir_Ethi\",\n",
        "    \"tr\": \"tur_Latn\",\n",
        "    \"uk\": \"ukr_Cyrl\",\n",
        "    \"ur\": \"urd_Arab\",\n",
        "    \"uz\": \"uzb_Latn\",\n",
        "    \"vi\": \"vie_Latn\",\n",
        "    \"xh\": \"xho_Latn\",\n",
        "    \"yo\": \"yor_Latn\",\n",
        "    \"zh-cn\": \"zho_Hans\",\n",
        "    \"zh-tw\": \"zho_Hant\",\n",
        "    \"zu\": \"zul_Latn\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2_L_pSePxT-",
        "outputId": "8aa078f5-742f-414b-ad5e-0b2b30c39867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the path to the pdf/content/sodapdf-converted.pdf\n",
            "Extracting text from the pdf\n",
            "Text extraction completed!\n",
            "Sure! Here's a brief conversation between two people:  \n",
            "  \n",
            "---  \n",
            "  \n",
            "**Alex:** Hey Taylor, how was your weekend?  \n",
            "  \n",
            "**Taylor:** Hi Alex! It was great, thanks for asking. I went hiking on Saturday and just relaxed at home on \n",
            "Sunday. How about you?  \n",
            "  \n",
            "**Alex:** That sounds awesome! I spent most of my weekend catching up on some work and watched a \n",
            "few movies.   \n",
            "  \n",
            "**Taylor:** Nice, any recommendations for the movies?  \n",
            "  \n",
            "**Alex:** Definitely! If you haven't seen *Inception*, it's a must-watch. And I really enjoyed *The Grand \n",
            "Budapest Hotel* too.  \n",
            "  \n",
            "**Taylor:** I'll check those out. Thanks for the suggestions!   \n",
            "  \n",
            "**Alex:** No problem. Let me know what you think once you've watched them!  \n",
            "  \n",
            "**Taylor:** Will do. See you around!  \n",
            "  \n",
            "**Alex:** See you!  \n",
            "  \n",
            "---  \n",
            "  \n",
            "Feel free to adjust the context or add any specific details if needed!\n",
            "\n",
            "Detecting source language\n",
            "Detected language: en (eng_Latn)\n",
            "\n",
            "Available target languages:\n",
            "- af (afr_Latn)\n",
            "- am (amh_Ethi)\n",
            "- ar (arb_Arab)\n",
            "- as (asm_Beng)\n",
            "- az (aze_Latn)\n",
            "- be (bel_Cyrl)\n",
            "- bg (bul_Cyrl)\n",
            "- bn (ben_Beng)\n",
            "- bs (bos_Latn)\n",
            "- ca (cat_Latn)\n",
            "- ceb (ceb_Latn)\n",
            "- cs (ces_Latn)\n",
            "- cy (cym_Latn)\n",
            "- da (dan_Latn)\n",
            "- de (deu_Latn)\n",
            "- el (ell_Grek)\n",
            "- en (eng_Latn)\n",
            "- es (spa_Latn)\n",
            "- et (est_Latn)\n",
            "- fa (pes_Arab)\n",
            "- fi (fin_Latn)\n",
            "- fr (fra_Latn)\n",
            "- gu (guj_Gujr)\n",
            "- ha (hau_Latn)\n",
            "- he (heb_Hebr)\n",
            "- hi (hin_Deva)\n",
            "- hr (hrv_Latn)\n",
            "- hu (hun_Latn)\n",
            "- hy (hye_Armn)\n",
            "- id (ind_Latn)\n",
            "- is (isl_Latn)\n",
            "- it (ita_Latn)\n",
            "- ja (jpn_Jpan)\n",
            "- jv (jav_Latn)\n",
            "- ka (kat_Geor)\n",
            "- kk (kaz_Cyrl)\n",
            "- km (khm_Khmr)\n",
            "- kn (kan_Knda)\n",
            "- ko (kor_Hang)\n",
            "- lo (lao_Laoo)\n",
            "- lt (lit_Latn)\n",
            "- lv (lav_Latn)\n",
            "- mg (plt_Latn)\n",
            "- mk (mkd_Cyrl)\n",
            "- ml (mal_Mlym)\n",
            "- mn (khk_Cyrl)\n",
            "- mr (mar_Deva)\n",
            "- ms (zsm_Latn)\n",
            "- my (mya_Mymr)\n",
            "- ne (npi_Deva)\n",
            "- nl (nld_Latn)\n",
            "- no (nob_Latn)\n",
            "- or (ori_Orya)\n",
            "- pa (pan_Guru)\n",
            "- pl (pol_Latn)\n",
            "- ps (pus_Arab)\n",
            "- pt (por_Latn)\n",
            "- ro (ron_Latn)\n",
            "- ru (rus_Cyrl)\n",
            "- si (sin_Sinh)\n",
            "- sk (slk_Latn)\n",
            "- sl (slv_Latn)\n",
            "- so (som_Latn)\n",
            "- sq (sqi_Latn)\n",
            "- sr (srp_Cyrl)\n",
            "- su (sun_Latn)\n",
            "- sv (swe_Latn)\n",
            "- ta (tam_Taml)\n",
            "- te (tel_Telu)\n",
            "- tg (tgk_Cyrl)\n",
            "- th (tha_Thai)\n",
            "- ti (tir_Ethi)\n",
            "- tr (tur_Latn)\n",
            "- uk (ukr_Cyrl)\n",
            "- ur (urd_Arab)\n",
            "- uz (uzb_Latn)\n",
            "- vi (vie_Latn)\n",
            "- xh (xho_Latn)\n",
            "- yo (yor_Latn)\n",
            "- zh-cn (zho_Hans)\n",
            "- zh-tw (zho_Hant)\n",
            "- zu (zul_Latn)\n",
            "\n",
            "Select the target language (language code): hi\n",
            "\n",
            "Translating text....\n",
            "Translation completed!\n",
            "Translated_text:   ['निश्चित रूप से! यहाँ दो लोगों के बीच एक संक्षिप्त बातचीत है: ** एलेक्स:** हे टेलर, आपका सप्ताहांत कैसा था? ** टेलर:** हाय एलेक्स! यह बहुत अच्छा था, पूछने के लिए धन्यवाद. मैं शनिवार को घूम रहा था और रविवार को घर पर बस आराम कर रहा था. आप के बारे में क्या? ** एलेक्स:** यह अद्भुत लगता है. मैंने अपने सप्ताहांत का अधिकांश हिस्सा कुछ काम पर पकड़ लिया और कुछ फिल्मों को देखा. ** टेलर:** अच्छा, फिल्मों के लिए कोई सिफारिशें? ** एलेक्स:** निश्चित रूप से!']\n"
          ]
        }
      ],
      "source": [
        "#main function:\n",
        "def main():\n",
        "  pdf_path = input(\"Enter the path to the pdf\")\n",
        "\n",
        "  if  not os.path.exists(pdf_path):\n",
        "    print(f\"Error: the file at {pdf_path} does not exist\")\n",
        "    return\n",
        "\n",
        "  #text extraction\n",
        "  print(\"Extracting text from the pdf\")\n",
        "  text = extract_text_from_pdf(pdf_path)\n",
        "  print(\"Text extraction completed!\")\n",
        "  print(text)\n",
        "\n",
        "  #detecting source language\n",
        "  print(\"\\nDetecting source language\")\n",
        "\n",
        "  detected_lang = detect_language(text)\n",
        "  if detected_lang:\n",
        "    source_lang_code = lang_code_map.get(detected_lang)\n",
        "    if source_lang_code:\n",
        "      print(f\"Detected language: {detected_lang} ({source_lang_code})\")\n",
        "    else:\n",
        "      print(f\"Detected language: {detected_lang}, but no corresponding NLLB language code found\")\n",
        "      return\n",
        "  else:\n",
        "    print(\"Language detection failed\")\n",
        "\n",
        "\n",
        "  #select the target language\n",
        "  print(\"\\nAvailable target languages:\")\n",
        "  for lang, code in lang_code_map.items():\n",
        "    print(f\"- {lang} ({code})\")\n",
        "\n",
        "  target_lang = input(\"\\nSelect the target language (language code): \")\n",
        "\n",
        "  if target_lang not in lang_code_map.keys():\n",
        "    print(\"Error: Invalid language selection\")\n",
        "    return\n",
        "\n",
        "  #translate text\n",
        "  print(\"\\nTranslating text....\")\n",
        "  translated_text = translate_text_nllb(text , target_lang)\n",
        "  print(\"Translation completed!\")\n",
        "  print(\"Translated_text:  \" ,translated_text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IxWl9Qzv9T4",
        "outputId": "e0419d51-9089-4a1d-98ce-f21d59c71b3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['La vie est comme une boîte de chocolat']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, M2M100ForConditionalGeneration\n",
        "\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
        "\n",
        "text_to_translate = \"Life is like a box of chocolates\"\n",
        "model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n",
        "\n",
        "# translate to French\n",
        "gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n",
        "print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DfVlMPJyWQ-",
        "outputId": "7636538b-1b23-4728-97e9-a71c1d5b686d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['निश्चित रूप से! यहाँ दो लोगों के बीच एक संक्षिप्त बातचीत है: ** एलेक्स:** हे टेलर, आपका सप्ताहांत कैसा था? ** टेलर:** हाय एलेक्स! यह बहुत अच्छा था, पूछने के लिए धन्यवाद. मैं शनिवार को घूम रहा था और रविवार को घर पर बस आराम कर रहा था. आप के बारे में क्या? ** एलेक्स:** यह अद्भुत लगता है. मैंने अपने सप्ताहांत का अधिकांश हिस्सा कुछ काम पर पकड़ लिया और कुछ फिल्मों को देखा. ** टेलर:** अच्छा, फिल्मों के लिए कोई सिफारिशें? ** एलेक्स:** निश्चित रूप से!']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " ['निश्चित रूप से! यहाँ दो लोगों के बीच एक संक्षिप्त बातचीत है: ** एलेक्स:** हे टेलर, आपका सप्ताहांत कैसा था? ** टेलर:** हाय एलेक्स! यह बहुत अच्छा था, पूछने के लिए धन्यवाद. मैं शनिवार को घूम रहा था और रविवार को घर पर बस आराम कर रहा था. आप के बारे में क्या? ** एलेक्स:** यह अद्भुत लगता है. मैंने अपने सप्ताहांत का अधिकांश हिस्सा कुछ काम पर पकड़ लिया और कुछ फिल्मों को देखा. ** टेलर:** अच्छा, फिल्मों के लिए कोई सिफारिशें? ** एलेक्स:** निश्चित रूप से!']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VISe0Ub66Xut"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
